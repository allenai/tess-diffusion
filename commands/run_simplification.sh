DEBUG_params="--num_inference_diffusion_steps 10 --max_eval_samples 10 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --eval_steps 10"
PARAMS_FOR_LOCAL=" --save_total_limit 1 "


# debug
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name cc  --output_dir ${LOCAL_DIR}/outputs/paper_experiments/debug --per_device_train_batch_size=12 --per_device_eval_batch_size=64 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps 2 --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean" --eval_steps 10 --max_eval_samples 64  --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/cc/"

# data length=512
: '
num_inference_diffusion_steps=600
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir ${LOCAL_DIR}/outputs/paper_experiments/simplification_${num_inference_diffusion_steps} --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_eval_samples 96 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'
: '
# Running on all the data.
num_inference_diffusion_steps=200
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir ${LOCAL_DIR}/outputs/paper_experiments/simplification_${num_inference_diffusion_steps}_all_data --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'

######################################################
# self-conditioning with different setting.
######################################################
num_inference_diffusion_steps=1000
: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"

python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_before_weights true
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_before_weights true  --resume_from_checkpoint "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights/checkpoint-17000/"
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_before_weights true
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_before_weights true --resume_from_checkpoint ${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights/checkpoint-17000/
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100
'

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights/checkpoint-14000/"

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint ${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights/checkpoint-14000/

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights/checkpoint-14000/"

# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights/checkpoint-16000/"

######################################################
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/debug/" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100  --self_condition_mix_logits_before_weights true --self_condition "logits_mean"
######################################################

# Running the evals.
num_inference_diffusion_steps=1000
: '
python -m torch.distributed.launch --nproc_per_node 4  run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max/checkpoint-19000/"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_max" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_mean" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply" --load_states_in_eval_from_model_path true
'

: '
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights/checkpoint-18000" --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_before_weights true  --load_states_in_eval_from_model_path true
'
#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights/checkpoint-18000/"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_addition"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

# python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition/checkpoint-22000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --load_states_in_eval_from_model_path

: '
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights/checkpoint-18000" --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "${LOCAL_DIR}/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
'

#############################################################
# Tune different lrs with the best self-conditioing model
#############################################################
# NOTE: batch size is divided by two.
#${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_3e-5_no_wd_guidance_2.0_mask_tokenlearning_rate=5e-5 #, 5e-5, 1e-5, 2e-5
# num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 2 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/tune_lrs_simplification/lr_"${learning_rate} --per_device_train_batch_size=6 --per_device_eval_batch_size=24   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000  --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 --resume_from_checkpoint  ${LOCAL_DIR}/outputs/paper_experiments/tune_lrs_simplification/lr_5e-5/checkpoint-72000/ ${PARAMS_FOR_LOCAL}
#python -m torch.distributed.launch --nproc_per_node 2 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/tune_lrs_simplification/lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=6 --per_device_eval_batch_size=24   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96  --resume_from_checkpoint ${LOCAL_DIR}/outputs/paper_experiments/tune_lrs_simplification/lr_5e-5_no_wd/checkpoint-74000/ ${PARAMS_FOR_LOCAL}

#############################################################
# *** Train the best model, this is the reported results for wikilarge but we ean on checkpoint 400.***
# lr=3e-5 and 2e-5 can be the best.
learning_rate=2e-5 
num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"

# this is reported one
# evaluate wikilarge for different seeds.
model_name="simplification_results/ours_2e-5/checkpoint-400000"
num_inference_diffusion_steps=1000
seed=5
TEMPERATURE=1.0
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_predict --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd/seeds_10" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge" --temperature ${TEMPERATURE} --load_states_in_eval_from_model_path true --seed ${seed}  --generate_with_seed true


: '
for learning_rate in 2e-5
do
  for TOP_P in 0.9 0.95 0.99	  
  do
	  for TEMPERATURE in 1 2 4
	  do  
          model_name="simplification_results/ours_2e-5/checkpoint-400000"
          num_inference_diffusion_steps=1000
          python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_predict --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p ${TOP_P} --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge" --temperature ${TEMPERATURE} --load_states_in_eval_from_model_path true
  done
  done
done 
'
# ****** this is the reported eval on wikilarge ******
# evaluate for top-p=None
: '
learning_rate=2e-5
for TEMPERATURE in 1 2 4
do  
    model_name="simplification_results/ours_2e-5/checkpoint-400000"
    num_inference_diffusion_steps=1000
    python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_predict --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge" --temperature ${TEMPERATURE} --load_states_in_eval_from_model_path true
done
'



##########################################################
# Ablation for the self-conditioning methods.
##########################################################
# TODO: run this.
learning_rate=2e-5 
num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/self_condition_ablation_results/simplification_lr_"${learning_rate}"_no_wd_logits_mean_mix_before_weights" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean_self_condition_mix_before_weights"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3  --dataset_folder "wikilarge"

# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/self_condition_ablation_results/simplification_lr_"${learning_rate}"_no_wd_logits" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits"   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3  --dataset_folder "wikilarge"
##########################################################
# DEBUG
#python run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/debug" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits"   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0  --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --overwrite_output_dir --eval_steps 2  --num_inference_diffusion_steps 10

##########################################################
# Training on wiki_alignment.
# ****** this is selected *******
learning_rate=3e-5 
num_inference_diffusion_steps=1000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

#------------------------------------------------------------------------------------------
# Train the wiki-alignment base model with classifier-free guidance.
guidance=2.0
learning_rate=3e-5 
num_inference_diffusion_steps=1000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance} 
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance}"_inputs_empty_token_repeat" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "empty_token"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance}"_inputs_noisy_simplex" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "noisy_simplex"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance}"_mask_token" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance}"_mask_token_inputs" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true --classifier_free_simplex_inputs true

# ------------------------------------------------------------------------------------------
# evaluate classifier-free guidance on a limited examples with and without softmax to find the best bet.

learning_rate=3e-5 
num_inference_diffusion_steps=1000
# reference for guidance = 1 still model trained with guidance.
# model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0/checkpoint-80000/"
# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path}"/examples_500"  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale 1.0 --max_predict_samples 800

# this is the original model
#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_3e-5_no_wd/checkpoint-80000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path}"/examples_500"  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale 1.0 --max_predict_samples 800

: '
for guidance in  2 4 8 10 16
do
  model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0/checkpoint-80000/"
  python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path}"/examples_500"  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --max_predict_samples 800
done 
'
: '
# **** this is the best version ****
# for guidance in  2 4 8 10 16
for guidance in  1.5
do
  model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0/checkpoint-80000/"
  python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path}"/examples_500/softmax"  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --max_predict_samples 800 --guidance_softmax_combination true
done 
'

: '
# ---------------
for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_inputs_empty_token_repeat/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500/" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "empty_token" --max_predict_samples 800
done 

for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_inputs_empty_token_repeat/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500/softmax" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "empty_token" --max_predict_samples 800  --guidance_softmax_combination true
done 

# ---------------
# does not makes sense.
# for guidance in 2 4 8 10 16
# do
# model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_inputs_noisy_simplex/checkpoint-80000/"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "noisy_simplex" --max_predict_samples 800
# done 

# for 2 4 8 10 16
# do
# model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_inputs_noisy_simplex/checkpoint-80000/"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500/softmax" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "noisy_simplex" --max_predict_samples 800  --guidance_softmax_combination true
# done 

# ---------------
for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_mask_token/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true --max_predict_samples 800
done

for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_mask_token/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500/softmax" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true --max_predict_samples 800 --guidance_softmax_combination true
done

# ---------------
for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_mask_token_inputs/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true --classifier_free_simplex_inputs true  --max_predict_samples 800
done 


for guidance in 2 4 8 10 16
do
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_2.0_mask_token_inputs/checkpoint-80000/"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/examples_500/softmax" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}   --classifier_free_uncond_input "empty_token" --empty_token_be_mask true --classifier_free_simplex_inputs true  --max_predict_samples 800 --guidance_softmax_combination true
done 
'
# ------------------------------------------------------------------------------------------
: '
# evaluate the classifier-free guidance.
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_3e-5_no_wd_guidance_2.0_inputs_empty_token_old/checkpoint-80000/"
for guidance in 2 4 6 8 10 16
do
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance}"_inputs_empty_token" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}  --classifier_free_simplex_inputs true  --classifier_free_uncond_input  "empty_token"
done
'




: '
# Evaluate the above model.
for learning_rate in 3e-5 #2e-5 3e-5 5e-5
do
  for TOP_P in 0.9 0.95 0.99	  
  do
	  for TEMPERATURE in 1  #2 4
	  do  
model_name="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_eval  --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p ${TOP_P} --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --load_states_in_eval_from_model_path true --temperature ${TEMPERATURE}
  done
  
  # ******* this is reported with lr=3e-5 for wiki-alignment *********
  # top-p = None
  TEMPERATURE=1
  model_name="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
  python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_eval  --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --load_states_in_eval_from_model_path true --temperature ${TEMPERATURE}
  done
done  
'


# Set more max-steps and iters => did not worked.
# learning_rate=3e-5 
# num_inference_diffusion_steps=1000
# max_steps=100000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr_steps/ours_lr_"${learning_rate}"_no_wd_steps_"${max_steps} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

# Run the evaluation of the above model.
#learning_rate=1e-5 
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "checkpoint-35000/" --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"
######################################################

# Ablation for the self-condition.
learning_rate=3e-5 
num_inference_diffusion_steps=1000
max_steps=60000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits_mean_ours_mix_before_weights" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"


#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits" --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"


#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_no_self_condition" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"


# run the eval for top-p=None
#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits_mean_ours_mix_before_weights/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits" --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_no_self_condition/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"




# I copied the checkpoints from the S3.
# run the eval for 40k steps.
#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits_mean_ours_mix_before_weights/checkpoint-40000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits/checkpoint-40000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits" --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"

#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_no_self_condition/checkpoint-40000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"




# --------------------------------------------------------------
#learning_rate=2e-5 
#num_inference_diffusion_steps=1000
#max_steps=300000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_base_lr_"${learning_rate}"_no_wd_max_steps_"${max_steps} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"

#max_steps=200000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wikilarge  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_base_lr_"${learning_rate}"_no_wd_max_steps_"${max_steps} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"

# eval the trained models.
learning_rate=2e-5 
num_inference_diffusion_steps=1000
#max_steps=300000
#model_path="${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_base_lr_"${learning_rate}"_no_wd_max_steps_"${max_steps}"/checkpoint-300000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wikilarge  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"

#max_steps=200000
#model_path="${LOCAL_DIR}/outputs/paper_experiments/simplification_results/ours_base_lr_"${learning_rate}"_no_wd_max_steps_"${max_steps}"/checkpoint-200000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wikilarge  --output_dir ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"


# --------------------------------------------------------------
# ablation on the number of inference steps on wiki-alignment. max samples = 1k
#learning_rate=3e-5 
#num_inference_diffusion_steps=10
#model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/inference_steps_ablation/step_"${num_inference_diffusion_steps}  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/" --max_predict_samples 1000


: '
# ablation on number of inference steps on all data
learning_rate=3e-5 
for num_inference_diffusion_steps in 10 100
do 
model_path="${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path}"/inference_steps_ablation_all_data/step_"${num_inference_diffusion_steps}  --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"
done
'

# run the wiki-alignment on the checkpoint model from ul2 variable with length=256.
learning_rate=3e-5 
num_inference_diffusion_steps=1000
model_path="${LOCAL_DIR}/outputs/paper_experiments/ul2_variable_self_condition_mean_mix_before_weights_base_size/checkpoint-20000"
max_steps=60000 # 80000
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "${LOCAL_DIR}/outputs/paper_experiments/wiki_alignment_tuned_from_ul2_variable_len_256_checkpoint/ours_lr_"${learning_rate}"_no_wd_max_steps_"${max_steps} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "${LOCAL_DIR}/simplex-diffusion/datasets/wiki_alignment/"
